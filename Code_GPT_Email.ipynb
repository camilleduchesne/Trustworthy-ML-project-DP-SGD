{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e23dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107530af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camilleduchesne/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import ngrams \n",
    "# Check for TensorFlow GPU access\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "# See TensorFlow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb3e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import Sequence, Lowercase\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import GPT2TokenizerFast, GPT2Config,TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cb9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSplit(tup):\n",
    "    n = len(tup)\n",
    "    return tup[0 : (n - 1)]\n",
    "\n",
    "def labelSplit(tup):\n",
    "    n = len(tup)\n",
    "    return tup[n - 1]\n",
    "\n",
    "# get word from dictionary ID\n",
    "def getWord(d, i):\n",
    "    return list(d.keys())[list(d.values()).index(i)]\n",
    "\n",
    "#this function generates random numbers for the secret\n",
    "def generateSecret(length, size):\n",
    "    secret = \"\"\n",
    "    for i in range(length):\n",
    "        a = randint(0, size)\n",
    "        if a < 10:\n",
    "            a = \"0\" + str(a)\n",
    "        a = str(a)\n",
    "        secret = secret + a + \" \"\n",
    "    \n",
    "    return secret[:-1]\n",
    "\n",
    "#allows us to add every secret permutation to the \"test\" data set, allowing us to see what probability the model assigns to every value of r\n",
    "def enumerateSecrets(length, size, rid, pref):\n",
    "    d = []\n",
    "    \n",
    "    if length == 1:\n",
    "        for i in range(size):\n",
    "            a = pref + str(i)\n",
    "            d.append({'id' : rid,\n",
    "                      'text' : a,\n",
    "                      'noPunc' : a,\n",
    "                      'splchk' : a})\n",
    "            rid += 1\n",
    "    \n",
    "    if length == 2:\n",
    "        for i in range(size):\n",
    "            a = pref + str(i)\n",
    "            for j in range(size):\n",
    "                b = a + \" \" + str(j)\n",
    "                d.append({'id' : rid,\n",
    "                          'text' : b,\n",
    "                          'noPunc' : b,\n",
    "                          'splchk' : b})\n",
    "                rid += 1\n",
    "                \n",
    "    return d, rid\n",
    "\n",
    "def numericProbs_gpt(x, size, dictionary, gramSize, model, index): \n",
    "    xn = np.zeros((1, gramSize), dtype = float)\n",
    "    for k in range(gramSize):\n",
    "        xn[0][k] = x[index][k]\n",
    "\n",
    "    p0 = model.predict(xn)[0]\n",
    "    \n",
    "    numericProbs_gpt = np.zeros((size), dtype = float)\n",
    "    \n",
    "    for j in range(size):\n",
    "        a = str(j)\n",
    "        numericProbs_gpt[j] = p0[dictionary[a]]\n",
    "        \n",
    "    return numericProbs_gpt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5a0c1",
   "metadata": {},
   "source": [
    "## Data Cleaning & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5009b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from random import randint\n",
    "\n",
    "def cleanSMS(sms):\n",
    "    \n",
    "    # leetspeak\n",
    "    sms = re.sub(\"[\\.,]\", \" \", sms)\n",
    "    sms = re.sub(\" {2,}\", \" \", sms)\n",
    "    sms = re.sub(\" 2 \", \" to \", sms)\n",
    "    sms = re.sub(\" 4 | fr \", \" for \", sms)\n",
    "    \n",
    "    sms = re.sub(\" abt \", \" about \", sms)\n",
    "    sms = re.sub(\" aft \", \" after \", sms)\n",
    "    sms = re.sub(\" ard \", \" around \", sms)\n",
    "    \n",
    "    sms = re.sub(\" ar \", \" all right \", sms)\n",
    "    sms = re.sub(\" ar$\", \" all right\", sms)\n",
    "    \n",
    "    sms = re.sub(\" b \", \" be \", sms)\n",
    "    sms = re.sub(\" bcz \", \" because \", sms)\n",
    "    sms = re.sub(\" bday \", \" birthday\", sms)\n",
    "    sms = re.sub(\" brin \", \" bring \", sms)\n",
    "    \n",
    "    sms = re.sub(\" btw \", \" by the way \", sms)\n",
    "    sms = re.sub(\" btw$\", \" by the way\", sms)\n",
    "    \n",
    "    sms = re.sub(\" buk \", \" book \", sms)\n",
    "    \n",
    "    sms = re.sub(\" c \", \" see \", sms)\n",
    "    sms = re.sub(\"^c \", \"see \", sms)\n",
    "    \n",
    "    sms = re.sub(\" coz | cuz | cos \", \" cause \", sms)\n",
    "    sms = re.sub(\"^coz |^cuz |^cos \", \"cause \", sms)\n",
    "    \n",
    "    sms = re.sub(\" da \", \" the \", sms)\n",
    "    sms = re.sub(\" dat \", \" that \", sms)\n",
    "    \n",
    "    sms = re.sub(\" den \", \" then \", sms)\n",
    "    sms = re.sub(\"^den \", \"then \", sms)\n",
    "    sms = re.sub(\" den$\", \" then\", sms)\n",
    "    \n",
    "    sms = re.sub(\" dint? \", \" did not \", sms)\n",
    "    \n",
    "    sms = re.sub(\" dis \", \" this \", sms)\n",
    "    sms = re.sub(\" dis$\", \" this\", sms)\n",
    "    \n",
    "    sms = re.sub(\" dem | dm \", \" them \", sms)\n",
    "    sms = re.sub(\" dey \", \" they \", sms)\n",
    "    sms = re.sub(\"^dey \", \"they \", sms)\n",
    "    sms = re.sub(\" dnt \", \" do not \", sms)\n",
    "    \n",
    "    sms = re.sub(\" dun | don \", \" do not \", sms)\n",
    "    sms = re.sub(\"^dun |^don \", \"do not \", sms)\n",
    "    sms = re.sub(\" dun$| don$\", \" do not\", sms)\n",
    "    \n",
    "    sms = re.sub(\" e \", \" the \", sms)\n",
    "    sms = re.sub(\" esp \" , \" especially \", sms)\n",
    "    sms = re.sub(\" enuff \", \" enough \", sms)\n",
    "    sms = re.sub(\" frens \", \" friends \", sms)\n",
    "    \n",
    "    sms = re.sub(\" fren \" , \" friend \", sms)\n",
    "    sms = re.sub(\" fren$\", \" fren\", sms)\n",
    "    \n",
    "    sms = re.sub(\" frm \", \" from \", sms)\n",
    "    \n",
    "    sms = re.sub(\" gd \", \" good \", sms)\n",
    "    sms = re.sub(\"^gd \", \"good \", sms)\n",
    "    sms = re.sub(\" gd$\", \" good\", sms)\n",
    "    \n",
    "    sms = re.sub(\" gn \", \" good night \", sms)\n",
    "    sms = re.sub(\"^gn \", \"good night \", sms)\n",
    "    sms = re.sub(\" gn$\", \" good night\", sms)\n",
    "    \n",
    "    sms = re.sub(\"^hai \", \"hey \", sms)\n",
    "    \n",
    "    sms = re.sub(\" haf | hv | hav \", \" have \", sms)\n",
    "    sms = re.sub(\" haf$| hv$| hav$\", \" have\", sms)\n",
    "    \n",
    "    sms = re.sub(\" haven \", \" have not \", sms)\n",
    "    \n",
    "    sms = re.sub(\" hse \", \" house \", sms)\n",
    "    sms = re.sub(\" hse$\", \" house\", sms)\n",
    "    sms = re.sub(\" hw \", \" homework \", sms)\n",
    "    sms = re.sub(\"^hw \", \"how \", sms)\n",
    "    \n",
    "    sms = re.sub(\" i ll \", \" i will \", sms)\n",
    "    sms = re.sub(\"^i ll \", \"i will \", sms)\n",
    "    sms = re.sub(\" i ve \", \" i have \", sms)\n",
    "    sms = re.sub(\"^i ve \", \"i have \", sms)\n",
    "    \n",
    "    sms = re.sub(\" juz | jus | jos \", \" just \", sms)\n",
    "    sms = re.sub(\"^juz |^jus |^jos \", \"just \", sms)\n",
    "    \n",
    "    sms = re.sub(\"kd \", \"ked \", sms)\n",
    "    sms = re.sub(\" knw \", \" know \", sms)\n",
    "    \n",
    "    sms = re.sub(\" lar | lter \", \" later \", sms)\n",
    "    sms = re.sub(\" lar$| lter$\", \" later\", sms)\n",
    "    sms = re.sub(\"^lar |^lter \", \"later \", sms)\n",
    "    \n",
    "    sms = re.sub(\" lib \", \" library \", sms)\n",
    "    sms = re.sub(\" lib$\", \" library\", sms)\n",
    "    \n",
    "    sms = re.sub(\" lect \", \" lecture \", sms)\n",
    "    sms = re.sub(\"^ll \", \"i will \", sms)\n",
    "    sms = re.sub(\" lyk \", \" like \", sms)\n",
    "    sms = re.sub(\" m \", \" am \", sms)\n",
    "    sms = re.sub(\"^m \", \"i am \", sms)\n",
    "    sms = re.sub(\" mayb \", \" maybe \", sms)\n",
    "    sms = re.sub(\" meh \", \" me \", sms)\n",
    "    sms = re.sub(\" msg \", \" message \", sms)\n",
    "    sms = re.sub(\" neva \", \" never \", sms)\n",
    "    sms = re.sub(\" mum \", \" mom \", sms)\n",
    "    sms = re.sub(\" muz \", \" must \", sms)\n",
    "    sms = re.sub(\" n \", \" and \", sms)\n",
    "    sms = re.sub(\"nd \", \"ned \", sms)\n",
    "    sms = re.sub(\" nite \", \" night \", sms)\n",
    "    sms = re.sub(\" noe \", \" know \", sms)\n",
    "    \n",
    "    sms = re.sub(\" nt \", \" not \", sms)\n",
    "    sms = re.sub(\"^nt \", \"not \", sms)\n",
    "    \n",
    "    sms = re.sub(\" nvm \", \" never mind \", sms)\n",
    "    sms = re.sub(\" nvr \", \" never \", sms)\n",
    "    sms = re.sub(\" nw \", \" now \", sms)\n",
    "    \n",
    "    sms = re.sub(\" nxt \", \" next \", sms)\n",
    "    sms = re.sub(\"^nxt \", \"next \", sms)\n",
    "    \n",
    "    sms = re.sub(\" okie | ok | k \", \" okay \", sms)\n",
    "    sms = re.sub(\"^okie |^ok |^k \", \"okay \", sms)\n",
    "    sms = re.sub(\" okie$| ok$| k$\", \" okay\", sms)\n",
    "    \n",
    "    sms = re.sub(\" oredi | alr \", \" already \", sms)\n",
    "    sms = re.sub(\" oredi$| alr$\", \" already\", sms)\n",
    "    \n",
    "    sms = re.sub(\" oso \", \" also \", sms)\n",
    "    \n",
    "    sms = re.sub(\" plz \", \" please \", sms)\n",
    "    sms = re.sub(\"^plz \", \"please \", sms)\n",
    "    sms = re.sub(\" plz$\", \" please\", sms)\n",
    "    \n",
    "    sms = re.sub(\" pple? \", \" people \", sms)\n",
    "    \n",
    "    sms = re.sub(\" pg \", \" page \", sms)\n",
    "    sms = re.sub(\" pg$\", \" page\", sms)\n",
    "    \n",
    "    sms = re.sub(\" r \", \" are \", sms)\n",
    "    sms = re.sub(\"^r \", \"are \", sms)\n",
    "    sms = re.sub(\" r$\", \" are\", sms)\n",
    "    \n",
    "    sms = re.sub(\" rem \", \" remember \", sms)\n",
    "    sms = re.sub(\" rite \", \" right \", sms)\n",
    "    \n",
    "    sms = re.sub(\" rly \", \" really \", sms)\n",
    "    sms = re.sub(\"^rly \", \"really \", sms)\n",
    "    sms = re.sub(\" rly$\", \" really\", sms)\n",
    "    \n",
    "    sms = re.sub(\" ru \", \" are you \", sms)\n",
    "    sms = re.sub(\" s \", \" is \", sms)\n",
    "    sms = re.sub(\"^s \", \"its \", sms)\n",
    "    \n",
    "    sms = re.sub(\" sch \", \" school \", sms)\n",
    "    sms = re.sub(\" sch$\", \" school\", sms)\n",
    "    \n",
    "    sms = re.sub(\" shd | shld \", \" should \", sms)\n",
    "    sms = re.sub(\" slp \", \" sleep \", sms)\n",
    "    \n",
    "    sms = re.sub(\" sme\", \" some\", sms)\n",
    "    sms = re.sub(\"^sme\", \"some\", sms)\n",
    "    \n",
    "    sms = re.sub(\" smth \", \" something \", sms)\n",
    "    \n",
    "    sms = re.sub(\" tat \", \" that \", sms)\n",
    "    sms = re.sub(\"^tat \", \"that \", sms)\n",
    "    sms = re.sub(\" tat$\", \" that\", sms)\n",
    "    \n",
    "    sms = re.sub(\" tmr | tml \", \" tomorrow \", sms)\n",
    "    sms = re.sub(\"^tmr |^tml \", \"tomorrow \", sms)\n",
    "    sms = re.sub(\" tmr$| tml$\", \" tomorrow\", sms)\n",
    "    \n",
    "    sms = re.sub(\" thanx \", \" thanks \", sms)\n",
    "    sms = re.sub(\" thanx$\", \" thanks\", sms)\n",
    "    sms = re.sub(\"^thanx \", \"thanks \", sms)\n",
    "    \n",
    "    sms = re.sub(\" thgt \", \" thought \", sms)\n",
    "    sms = re.sub(\" thk | thnk \", \" think \", sms)\n",
    "    \n",
    "    sms = re.sub(\" tis \", \" this \", sms)\n",
    "    sms = re.sub(\" tot \" , \" thought \", sms)\n",
    "    sms = re.sub(\" ttyl$\", \" talk to you later\", sms)\n",
    "    \n",
    "    sms = re.sub(\" tym \", \" time \", sms)\n",
    "    sms = re.sub(\" tym\", \" time\", sms)\n",
    "    \n",
    "    sms = re.sub(\" [uüü] \", \" you \", sms)\n",
    "    sms = re.sub(\"^[uüü] \", \"you \", sms)\n",
    "    sms = re.sub(\" [uüü]$\", \" you\", sms)\n",
    "    \n",
    "    sms = re.sub(\" ur \", \" your \", sms)\n",
    "    sms = re.sub(\" v \", \" very \", sms)\n",
    "    sms = re.sub(\" vil \", \" will \", sms)\n",
    "    sms = re.sub(\"^ve \", \"i have \", sms)\n",
    "    sms = re.sub(\" wan \", \" want \", sms)\n",
    "    sms = re.sub(\" w \", \" with \", sms)\n",
    "    \n",
    "    sms = re.sub(\" wana \", \" wanna \", sms)\n",
    "    sms = re.sub(\"^wana \", \"wanna \", sms)\n",
    "    \n",
    "    sms = re.sub(\" wat \", \" what \", sms)\n",
    "    sms = re.sub(\"^wat \", \"what \", sms)\n",
    "    sms = re.sub(\" wat$\", \" what\", sms)\n",
    "    \n",
    "    sms = re.sub(\" wen \", \" when \", sms)\n",
    "    sms = re.sub(\"^wen \", \"when \", sms)\n",
    "    \n",
    "    sms = re.sub(\" wif | wid | wth \", \" with \", sms)\n",
    "    sms = re.sub(\"^wif |^wid |^wth \", \"with \", sms)\n",
    "    sms = re.sub(\" wif$| wid$| wth$\", \" with\", sms)\n",
    "    \n",
    "    sms = re.sub(\" wk \", \" week \", sms)\n",
    "\n",
    "    sms = re.sub(\" wun \", \" wont \", sms)\n",
    "    \n",
    "    sms = re.sub(\" y \", \" why \", sms)\n",
    "    sms = re.sub(\"^y \", \"why \", sms)\n",
    "    sms = re.sub(\" y$\", \" why\", sms)\n",
    "    \n",
    "    sms = re.sub(\"yup\", \"yep\", sms)\n",
    "\n",
    "    # remove laughter and smiles\n",
    "    sms = re.sub(\" d \", \" \", sms)\n",
    "    sms = re.sub(\" d$\", \"\", sms)\n",
    "    sms = re.sub(\"^d \", \"\", sms)\n",
    "    sms = re.sub(\" ha \", \" \", sms)\n",
    "    sms = re.sub(\"^ha \", \"\", sms)\n",
    "    sms = re.sub(\" ha$, \", \"\", sms)\n",
    "    sms = re.sub(\" lor \", \" \", sms)\n",
    "    sms = re.sub(\" lor$\", \"\", sms)\n",
    "    sms = re.sub(\" lols? \", \" \", sms)\n",
    "    sms = re.sub(\"^lols? \", \"\", sms)\n",
    "    sms = re.sub(\" lols?$\", \"\", sms)\n",
    "    sms = re.sub(\"a*(ha){2,}h*\", \"\", sms)\n",
    "    sms = re.sub(\" hee \", \" \", sms)\n",
    "    sms = re.sub(\"^hee \", \"\", sms)\n",
    "    sms = re.sub(\" hee$\", \"\", sms)\n",
    "    \n",
    "    # remove words I don't understand\n",
    "    sms = re.sub(\" lei \", \" \", sms)\n",
    "    sms = re.sub(\"^lei \", \" \", sms)\n",
    "    sms = re.sub(\" lei$\", \" \", sms)\n",
    "    \n",
    "    # standardize most '-ing' to '-in'\n",
    "    sms = re.sub(\"(?<=[bdfghklmnoprstvwy])ing(?= )\", \"in\", sms)\n",
    "    sms = re.sub(\"(?<=[bdfghklmnoprstvwy])ing$\", \"in\", sms)\n",
    "    \n",
    "    # force spaces between comma- or period-separated words\n",
    "    sms = re.sub(\"(?<=[^ ])[\\.,](?=[^ ])\", \" \", sms)\n",
    "    \n",
    "    return sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720f7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRaw = pd.read_csv('Email_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2a824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365397, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRaw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8b34d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>pallen nonprivileged pst here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>pallen nonprivileged pst travelin to have a bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>pallen nsf test successful way to go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>pallen nsf randy can you sened me a schedule o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>pallen nsf lets shoot for tuesday at</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        id  \\\n",
       "0           0     allen-p/_sent_mail/1.   \n",
       "1           1    allen-p/_sent_mail/10.   \n",
       "2           2   allen-p/_sent_mail/100.   \n",
       "3           3  allen-p/_sent_mail/1000.   \n",
       "4           4  allen-p/_sent_mail/1001.   \n",
       "\n",
       "                                                text  \n",
       "0      pallen nonprivileged pst here is our forecast  \n",
       "1  pallen nonprivileged pst travelin to have a bu...  \n",
       "2               pallen nsf test successful way to go  \n",
       "3  pallen nsf randy can you sened me a schedule o...  \n",
       "4               pallen nsf lets shoot for tuesday at  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRaw = pd.DataFrame(dataRaw)\n",
    "dataRaw.rename(columns = {'file':'id','message':'text'}, inplace = True)\n",
    "dataRaw['text'] = dataRaw['text'].astype(\"str\")\n",
    "dataRaw['text'] = dataRaw['text'].map(lambda x: x.lstrip(\"{'$':-\").rstrip(\"-'}\"))\n",
    "dataRaw['text'] = dataRaw['text'].str.replace(r\"'\", \"\")\n",
    "dataRaw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62e0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRaw['vocabsplit'] = dataRaw.text.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb13ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRaw['vocablen'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6dcd21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/632442385.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['vocablen'][i] = len(dataRaw['vocabsplit'][i])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataRaw)):\n",
    "    dataRaw['vocablen'][i] = len(dataRaw['vocabsplit'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227b9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRaw['length'] = dataRaw.text.str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25041de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRaw = dataRaw[dataRaw.length < 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "361d6e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42367"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf9055db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>vocabsplit</th>\n",
       "      <th>vocablen</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>pallen nonprivileged pst here is our forecast</td>\n",
       "      <td>[pallen, nonprivileged, pst, here, is, our, fo...</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>pallen nsf test successful way to go</td>\n",
       "      <td>[pallen, nsf, test, successful, way, to, go]</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>pallen nsf lets shoot for tuesday at</td>\n",
       "      <td>[pallen, nsf, lets, shoot, for, tuesday, at]</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>allen-p/_sent_mail/1002.</td>\n",
       "      <td>pallen nsf greg how about either next tuesday ...</td>\n",
       "      <td>[pallen, nsf, greg, how, about, either, next, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>allen-p/_sent_mail/1004.</td>\n",
       "      <td>pallen nsf any mornin between aned</td>\n",
       "      <td>[pallen, nsf, any, mornin, between, aned]</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365360</th>\n",
       "      <td>517353</td>\n",
       "      <td>zufferli-j/sent_items/56.</td>\n",
       "      <td>fax zuf howdy i hope all is well what is your ...</td>\n",
       "      <td>[fax, zuf, howdy, i, hope, all, is, well, what...</td>\n",
       "      <td>15</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365362</th>\n",
       "      <td>517357</td>\n",
       "      <td>zufferli-j/sent_items/6.</td>\n",
       "      <td>john zufferli   pst check out www segalmotorca...</td>\n",
       "      <td>[john, zufferli, pst, check, out, www, segalmo...</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365366</th>\n",
       "      <td>517361</td>\n",
       "      <td>zufferli-j/sent_items/63.</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365388</th>\n",
       "      <td>517391</td>\n",
       "      <td>zufferli-j/sent_items/90.</td>\n",
       "      <td>positions without sundance</td>\n",
       "      <td>[positions, without, sundance]</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365390</th>\n",
       "      <td>517393</td>\n",
       "      <td>zufferli-j/sent_items/92.</td>\n",
       "      <td>no eagle huh love jess</td>\n",
       "      <td>[no, eagle, huh, love, jess]</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42367 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                         id  \\\n",
       "0                0      allen-p/_sent_mail/1.   \n",
       "2                2    allen-p/_sent_mail/100.   \n",
       "4                4   allen-p/_sent_mail/1001.   \n",
       "5                5   allen-p/_sent_mail/1002.   \n",
       "7                7   allen-p/_sent_mail/1004.   \n",
       "...            ...                        ...   \n",
       "365360      517353  zufferli-j/sent_items/56.   \n",
       "365362      517357   zufferli-j/sent_items/6.   \n",
       "365366      517361  zufferli-j/sent_items/63.   \n",
       "365388      517391  zufferli-j/sent_items/90.   \n",
       "365390      517393  zufferli-j/sent_items/92.   \n",
       "\n",
       "                                                     text  \\\n",
       "0           pallen nonprivileged pst here is our forecast   \n",
       "2                    pallen nsf test successful way to go   \n",
       "4                    pallen nsf lets shoot for tuesday at   \n",
       "5       pallen nsf greg how about either next tuesday ...   \n",
       "7                      pallen nsf any mornin between aned   \n",
       "...                                                   ...   \n",
       "365360  fax zuf howdy i hope all is well what is your ...   \n",
       "365362  john zufferli   pst check out www segalmotorca...   \n",
       "365366                                                nan   \n",
       "365388                         positions without sundance   \n",
       "365390                             no eagle huh love jess   \n",
       "\n",
       "                                               vocabsplit  vocablen  length  \n",
       "0       [pallen, nonprivileged, pst, here, is, our, fo...         7      45  \n",
       "2            [pallen, nsf, test, successful, way, to, go]         7      36  \n",
       "4            [pallen, nsf, lets, shoot, for, tuesday, at]         7      36  \n",
       "5       [pallen, nsf, greg, how, about, either, next, ...        11      65  \n",
       "7               [pallen, nsf, any, mornin, between, aned]         6      34  \n",
       "...                                                   ...       ...     ...  \n",
       "365360  [fax, zuf, howdy, i, hope, all, is, well, what...        15      69  \n",
       "365362  [john, zufferli, pst, check, out, www, segalmo...         8      51  \n",
       "365366                                              [nan]         1       3  \n",
       "365388                     [positions, without, sundance]         3      26  \n",
       "365390                       [no, eagle, huh, love, jess]         5      22  \n",
       "\n",
       "[42367 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3310e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/2144697438.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['noPunc'] = dataRaw['text'].apply(lambda s: s.translate(str.maketrans('','', myPunc)).lower())\n",
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/2144697438.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['splchk'] = dataRaw['noPunc'].apply(cleanSMS)\n"
     ]
    }
   ],
   "source": [
    "#cleaning\n",
    "myPunc = '!\"#$%&\\()*+-/:;<=>?@[\\\\]^_`{|}~\\''\n",
    "dataRaw['noPunc'] = dataRaw['text'].apply(lambda s: s.translate(str.maketrans('','', myPunc)).lower())\n",
    "\n",
    "dataRaw['splchk'] = dataRaw['noPunc'].apply(cleanSMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3315d248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/2703484354.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['splchk'] = dataRaw['noPunc'].apply(cleanSMS)\n",
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/2703484354.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['splchk'] = dataRaw['splchk'].apply(cleanSMS)\n"
     ]
    }
   ],
   "source": [
    "#cleaning\n",
    "dataRaw['splchk'] = dataRaw['noPunc'].apply(cleanSMS)\n",
    "dataRaw['splchk'] = dataRaw['splchk'].apply(cleanSMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9acb4737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/2963461082.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['noPunc'] = dataRaw['text'].apply(lambda s: s.translate(str.maketrans('','', myPunc)).lower())\n",
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/2963461082.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['splchk'] = dataRaw['noPunc'].apply(cleanSMS)\n",
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/2963461082.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['splchk'] = dataRaw['noPunc'].apply(cleanSMS)\n",
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/2963461082.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataRaw['splchk'] = dataRaw['splchk'].apply(cleanSMS)\n"
     ]
    }
   ],
   "source": [
    "dataRaw = dataRaw[:30000]\n",
    "rootId = len(dataRaw)\n",
    "myPunc = '!\"#$%&\\()*+-/:;<=>?@[\\\\]^_`{|}~\\''\n",
    "dataRaw['noPunc'] = dataRaw['text'].apply(lambda s: s.translate(str.maketrans('','', myPunc)).lower())\n",
    "dataRaw['splchk'] = dataRaw['noPunc'].apply(cleanSMS)\n",
    "dataRaw['splchk'] = dataRaw['noPunc'].apply(cleanSMS)\n",
    "dataRaw['splchk'] = dataRaw['splchk'].apply(cleanSMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0f1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataRaw.to_csv('dataRaw.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34782b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataRaw = pd.read_csv('dataRaw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acf2d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootId = len(dataRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cc60276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>vocabsplit</th>\n",
       "      <th>vocablen</th>\n",
       "      <th>length</th>\n",
       "      <th>noPunc</th>\n",
       "      <th>splchk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>pallen nonprivileged pst here is our forecast</td>\n",
       "      <td>[pallen, nonprivileged, pst, here, is, our, fo...</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>pallen nonprivileged pst here is our forecast</td>\n",
       "      <td>pallen nonprivileged pst here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>pallen nsf test successful way to go</td>\n",
       "      <td>[pallen, nsf, test, successful, way, to, go]</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>pallen nsf test successful way to go</td>\n",
       "      <td>pallen nsf test successful way to go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>pallen nsf lets shoot for tuesday at</td>\n",
       "      <td>[pallen, nsf, lets, shoot, for, tuesday, at]</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>pallen nsf lets shoot for tuesday at</td>\n",
       "      <td>pallen nsf lets shoot for tuesday at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>allen-p/_sent_mail/1002.</td>\n",
       "      <td>pallen nsf greg how about either next tuesday ...</td>\n",
       "      <td>[pallen, nsf, greg, how, about, either, next, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "      <td>pallen nsf greg how about either next tuesday ...</td>\n",
       "      <td>pallen nsf greg how about either next tuesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>allen-p/_sent_mail/1004.</td>\n",
       "      <td>pallen nsf any mornin between aned</td>\n",
       "      <td>[pallen, nsf, any, mornin, between, aned]</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>pallen nsf any mornin between aned</td>\n",
       "      <td>pallen nsf any mornin between aned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242765</th>\n",
       "      <td>347016</td>\n",
       "      <td>nemec-g/all_documents/557.</td>\n",
       "      <td>ca data sheet kay please review the attached t...</td>\n",
       "      <td>[ca, data, sheet, kay, please, review, the, at...</td>\n",
       "      <td>12</td>\n",
       "      <td>65</td>\n",
       "      <td>ca data sheet kay please review the attached t...</td>\n",
       "      <td>ca data sheet kay please review the attached t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242773</th>\n",
       "      <td>347025</td>\n",
       "      <td>nemec-g/all_documents/5578.</td>\n",
       "      <td>gnemec nsf ken i made a few tweaks attached ar...</td>\n",
       "      <td>[gnemec, nsf, ken, i, made, a, few, tweaks, at...</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>gnemec nsf ken i made a few tweaks attached ar...</td>\n",
       "      <td>gnemec nsf ken i made a few tweaks attached ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242803</th>\n",
       "      <td>347062</td>\n",
       "      <td>nemec-g/all_documents/5611.</td>\n",
       "      <td>gnemec nsf here is the amendment we used</td>\n",
       "      <td>[gnemec, nsf, here, is, the, amendment, we, used]</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>gnemec nsf here is the amendment we used</td>\n",
       "      <td>gnemec nsf here is the amendment we used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242807</th>\n",
       "      <td>347067</td>\n",
       "      <td>nemec-g/all_documents/5616.</td>\n",
       "      <td>gnemec nsf per your notes</td>\n",
       "      <td>[gnemec, nsf, per, your, notes]</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>gnemec nsf per your notes</td>\n",
       "      <td>gnemec nsf per your notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242812</th>\n",
       "      <td>347073</td>\n",
       "      <td>nemec-g/all_documents/5621.</td>\n",
       "      <td>price amendment here is the amendment we used</td>\n",
       "      <td>[price, amendment, here, is, the, amendment, w...</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>price amendment here is the amendment we used</td>\n",
       "      <td>price amendment here is the amendment we used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                           id  \\\n",
       "0                0        allen-p/_sent_mail/1.   \n",
       "2                2      allen-p/_sent_mail/100.   \n",
       "4                4     allen-p/_sent_mail/1001.   \n",
       "5                5     allen-p/_sent_mail/1002.   \n",
       "7                7     allen-p/_sent_mail/1004.   \n",
       "...            ...                          ...   \n",
       "242765      347016   nemec-g/all_documents/557.   \n",
       "242773      347025  nemec-g/all_documents/5578.   \n",
       "242803      347062  nemec-g/all_documents/5611.   \n",
       "242807      347067  nemec-g/all_documents/5616.   \n",
       "242812      347073  nemec-g/all_documents/5621.   \n",
       "\n",
       "                                                     text  \\\n",
       "0           pallen nonprivileged pst here is our forecast   \n",
       "2                    pallen nsf test successful way to go   \n",
       "4                    pallen nsf lets shoot for tuesday at   \n",
       "5       pallen nsf greg how about either next tuesday ...   \n",
       "7                      pallen nsf any mornin between aned   \n",
       "...                                                   ...   \n",
       "242765  ca data sheet kay please review the attached t...   \n",
       "242773  gnemec nsf ken i made a few tweaks attached ar...   \n",
       "242803           gnemec nsf here is the amendment we used   \n",
       "242807                          gnemec nsf per your notes   \n",
       "242812      price amendment here is the amendment we used   \n",
       "\n",
       "                                               vocabsplit  vocablen  length  \\\n",
       "0       [pallen, nonprivileged, pst, here, is, our, fo...         7      45   \n",
       "2            [pallen, nsf, test, successful, way, to, go]         7      36   \n",
       "4            [pallen, nsf, lets, shoot, for, tuesday, at]         7      36   \n",
       "5       [pallen, nsf, greg, how, about, either, next, ...        11      65   \n",
       "7               [pallen, nsf, any, mornin, between, aned]         6      34   \n",
       "...                                                   ...       ...     ...   \n",
       "242765  [ca, data, sheet, kay, please, review, the, at...        12      65   \n",
       "242773  [gnemec, nsf, ken, i, made, a, few, tweaks, at...        13      64   \n",
       "242803  [gnemec, nsf, here, is, the, amendment, we, used]         8      40   \n",
       "242807                    [gnemec, nsf, per, your, notes]         5      25   \n",
       "242812  [price, amendment, here, is, the, amendment, w...         8      45   \n",
       "\n",
       "                                                   noPunc  \\\n",
       "0           pallen nonprivileged pst here is our forecast   \n",
       "2                    pallen nsf test successful way to go   \n",
       "4                    pallen nsf lets shoot for tuesday at   \n",
       "5       pallen nsf greg how about either next tuesday ...   \n",
       "7                      pallen nsf any mornin between aned   \n",
       "...                                                   ...   \n",
       "242765  ca data sheet kay please review the attached t...   \n",
       "242773  gnemec nsf ken i made a few tweaks attached ar...   \n",
       "242803           gnemec nsf here is the amendment we used   \n",
       "242807                          gnemec nsf per your notes   \n",
       "242812      price amendment here is the amendment we used   \n",
       "\n",
       "                                                   splchk  \n",
       "0           pallen nonprivileged pst here is our forecast  \n",
       "2                    pallen nsf test successful way to go  \n",
       "4                    pallen nsf lets shoot for tuesday at  \n",
       "5       pallen nsf greg how about either next tuesday ...  \n",
       "7                      pallen nsf any mornin between aned  \n",
       "...                                                   ...  \n",
       "242765  ca data sheet kay please review the attached t...  \n",
       "242773  gnemec nsf ken i made a few tweaks attached ar...  \n",
       "242803           gnemec nsf here is the amendment we used  \n",
       "242807                          gnemec nsf per your notes  \n",
       "242812      price amendment here is the amendment we used  \n",
       "\n",
       "[30000 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbe5c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "#we have 20% of the whole data known as dataRaw for test and it is knwon as dataRawT\n",
    "#then the othe 80% is known as dataRawR will soon split to valid and train\n",
    "mskTrain = np.random.rand(len(dataRaw)) < 0.8\n",
    "dataRawR = dataRaw[mskTrain]\n",
    "dataRawT = dataRaw[~mskTrain]\n",
    "\n",
    "# train-validation split\n",
    "mskVal = np.random.rand(len(dataRawR)) < 0.8\n",
    "#Here from the left 80% of the data in dataRawR we have 80% for training known as dataRawR and 20% for validation known as dataRawV\n",
    "dataRawV = dataRawR[~mskVal]\n",
    "dataRawR = dataRawR[mskVal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a429f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+---------------------------------------+\n",
      "| THANK YOU FOR USING THE SECRET SHARER |\n",
      "+---------------------------------------+\n",
      "\n",
      " True secrets inserted: 1\n",
      " False secrets inserted: 2\n",
      " Randomness space: 100\n",
      " Training epochs: 5\n",
      " Batch size: 256\n",
      " Secret text: 'my permanent code is 73 04'\n",
      "\n",
      "-----------------------------------------\n",
      "\n",
      "preparing data...\n"
     ]
    }
   ],
   "source": [
    "# 0. EXPERIMENTAL SETUP ====================================\n",
    "\n",
    "# how many copies of the secret do we insert?\n",
    "numTrueSecrets = 1\n",
    "# how many 'noisy' secrets do we insert?\n",
    "numFalseSecrets = 2\n",
    "# how many ticks are on our lock?\n",
    "numDistinctValues = 100\n",
    "# how long should we train the model?\n",
    "numEpochs = 5\n",
    "batchSize = 256\n",
    "\n",
    "# what form should the secret take?\n",
    "secretPref = \"my permanent code is \"\n",
    "seqLength = len(secretPref.split())\n",
    "gramSize = seqLength + 1\n",
    "\n",
    "# randomness space\n",
    "secretLength = 2\n",
    "bigR = numDistinctValues ** secretLength\n",
    "\n",
    "# generate a random secret\n",
    "#the next line generate the random number we want to add to the prefix\n",
    "secretText = generateSecret(secretLength, numDistinctValues)\n",
    "insertedSecret = secretPref + secretText\n",
    "\n",
    "print(\"\\n+---------------------------------------+\")\n",
    "print(\"| THANK YOU FOR USING THE SECRET SHARER |\")\n",
    "print(\"+---------------------------------------+\\n\")\n",
    "print(\" True secrets inserted:\", numTrueSecrets)\n",
    "print(\" False secrets inserted:\", numFalseSecrets)\n",
    "print(\" Randomness space:\", numDistinctValues)\n",
    "print(\" Training epochs:\", numEpochs)\n",
    "print(\" Batch size:\", batchSize)\n",
    "print(\" Secret text: '\", insertedSecret, \"'\\n\", sep = '')\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"\\npreparing data...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "767adda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/3492460920.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataRawT = dataRawT.append(d)\n",
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/3492460920.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataRawdct = dataRaw.append(d)\n"
     ]
    }
   ],
   "source": [
    "d, rootId = enumerateSecrets(secretLength, numDistinctValues, rootId, secretPref)\n",
    "#d here is all possible secrets\n",
    "#for the number of false secrets that we ordered it sample from d and make a data fram with name noise out of that \n",
    "# get some noise from these fake secret to add to training\n",
    "\n",
    "if numFalseSecrets > 0:\n",
    "    noise = [d[i] for i in sorted(random.sample(range(len(d)), numFalseSecrets))]\n",
    "    noiseDF = pd.DataFrame(noise)\n",
    "\n",
    "testSecret = pd.DataFrame(d);\n",
    "#test data has all possible secrets in it now\n",
    "dataRawT = dataRawT.append(d)\n",
    "#data for dct\n",
    "dataRawdct = dataRaw.append(d)\n",
    "\n",
    "# d = []\n",
    "# # several in training data\n",
    "# for i in range(numTrueSecrets):\n",
    "#     d.append({'id' : rootId,\n",
    "#               'text' : insertedSecret,\n",
    "#               'noPunc' : insertedSecret,\n",
    "#               'splchk' : insertedSecret})\n",
    "#     rootId += 1\n",
    "# #truesecrets add in train data\n",
    "# trainSecret = pd.DataFrame(d)\n",
    "#dataRawR = dataRawR.append(d)\n",
    "\n",
    "#also the noise is added to train set\n",
    "# if numFalseSecrets > 0:\n",
    "#     dataRawR = dataRawR.append(noiseDF)\n",
    "#the train set have true sectrets and false secrets while test set has all possible secrets\n",
    "\n",
    "# 2.4 SPLIT INTO OVERLAPPING SETS OF WORDS -----------000000\n",
    "\n",
    "d = []\n",
    "gid = 0\n",
    "for i in range(len(dataRawR)):\n",
    "    grams = ngrams(dataRawR.splchk.iloc[i].split(), gramSize)\n",
    "    for g in grams:\n",
    "        d.append({'id' : gid,\n",
    "                  'data' : g})   \n",
    "        gid += 1\n",
    "\n",
    "dataGramsR = pd.DataFrame(d)\n",
    "\n",
    "d = []\n",
    "for i in range(len(dataRawV)):\n",
    "    grams = ngrams(dataRawV.splchk.iloc[i].split(), gramSize)\n",
    "    for g in grams:\n",
    "        d.append({'id' : gid,\n",
    "                  'data' : g})   \n",
    "        gid += 1\n",
    "\n",
    "dataGramsV = pd.DataFrame(d)\n",
    "\n",
    "d = []\n",
    "for i in range(len(dataRawT)):\n",
    "    grams = ngrams(dataRawT.splchk.iloc[i].split(), gramSize)\n",
    "    for g in grams:\n",
    "        d.append({'id' : gid,\n",
    "                  'data' : g})   \n",
    "        gid += 1\n",
    "\n",
    "dataGramsT = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9240764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create functionc to turn Golshid's df to the right format for my tokenizer\n",
    "\n",
    "def prep_token(df):\n",
    "    df['data'] = df['data'].astype(str)\n",
    "    df['data']=df['data'].str.replace(\"(\",\"\") \n",
    "    df['data']=df['data'].str.replace(\")\",\"\")\n",
    "    df['data']=df['data'].str.replace(\",\",\" \")\n",
    "    df['data']=df['data'].str.replace(\"'\",\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f1df158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/3672755602.py:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['data']=df['data'].str.replace(\"(\",\"\")\n",
      "/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/3672755602.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['data']=df['data'].str.replace(\")\",\"\")\n"
     ]
    }
   ],
   "source": [
    "prep_train=prep_token(dataGramsR)\n",
    "prep_test=prep_token(dataGramsT)\n",
    "prep_val=prep_token(dataGramsV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c11be130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep_train=prep_train.drop(['codes','x','y'],axis=1)\n",
    "#prep_test=prep_test.drop(['codes', 'x','y'],axis=1)\n",
    "#prep_val=prep_val.drop(['codes', 'x','y'],axis=1)\n",
    "#prep_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48de1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up, change the column name to fit the rest of the code, and drop id\n",
    "prep_train=prep_train.drop(['id'],axis=1)\n",
    "prep_train.columns = ['text']\n",
    "\n",
    "prep_test=prep_test.drop(['id'],axis=1)\n",
    "prep_test.columns = ['text']\n",
    "\n",
    "prep_val=prep_val.drop(['id'],axis=1)\n",
    "prep_test.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8415337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up, change the column name to fit the rest of the code, and drop id\n",
    "#prep_train=prep_train.drop(['id'],axis=1)\n",
    "#prep_train.columns = ['text']\n",
    "\n",
    "#prep_test=prep_test.drop(['id'],axis=1)\n",
    "#prep_test.columns = ['text']\n",
    "\n",
    "#prep_val=prep_val.drop(['id'],axis=1)\n",
    "#prep_test.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "debccbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pallen  nsf  any  mornin  between</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nsf  any  mornin  between  aned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pallen  nsf  paula  million  is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nsf  paula  million  is  fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paula  million  is  fine  phillip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41838</th>\n",
       "      <td>permanent  code  is  99  97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41839</th>\n",
       "      <td>my  permanent  code  is  99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41840</th>\n",
       "      <td>permanent  code  is  99  98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41841</th>\n",
       "      <td>my  permanent  code  is  99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41842</th>\n",
       "      <td>permanent  code  is  99  99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41843 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text\n",
       "0      pallen  nsf  any  mornin  between\n",
       "1        nsf  any  mornin  between  aned\n",
       "2        pallen  nsf  paula  million  is\n",
       "3          nsf  paula  million  is  fine\n",
       "4      paula  million  is  fine  phillip\n",
       "...                                  ...\n",
       "41838        permanent  code  is  99  97\n",
       "41839        my  permanent  code  is  99\n",
       "41840        permanent  code  is  99  98\n",
       "41841        my  permanent  code  is  99\n",
       "41842        permanent  code  is  99  99\n",
       "\n",
       "[41843 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check, works like a charm!\n",
    "#prep_train\n",
    "prep_test\n",
    "#prep_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96cc5505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "4.17.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import datasets\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "pd.options.display.max_colwidth = 6000\n",
    "pd.options.display.max_rows = 400\n",
    "np.set_printoptions(suppress=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Log Level and suppress extensive tf warnings\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"info\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print(tf.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "859bc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow_hub as hub\n",
    "#import tensorflow_text as text\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "521350a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is based on the post-cleaned test set, for calculating the exposure metric, we need only the 4 first tokens\n",
    "#The function from above was thus updated\n",
    "#xt_inprocess = modif_test.values.tolist()\n",
    "xt_inprocess=prep_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b0e8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insure the right format to calculate the exposure for the test set\n",
    "def create_xt(lst):\n",
    "    res=[]\n",
    "    first_1=[]\n",
    "    final_first_1=[]\n",
    "    list_3 = []\n",
    "    text_file=[]\n",
    "    for i in range(0,len(lst)):\n",
    "        text_file=lst[i]\n",
    "        \n",
    "        for j in range(0,len(text_file)):\n",
    "            first_1=re.split(\" \", text_file[j])\n",
    "            final_first_1 = first_1[0:4]\n",
    "            joint_toget = [\" \".join([str(item) for item in final_first_1])]\n",
    "            res.append(joint_toget)\n",
    "    \n",
    "    #new_list = [lst[j:j+1] for j in range(len(lst)-stopping,len(lst))] \n",
    "    #list_3.extend(res + new_list)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b485e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_CD=create_xt(xt_inprocess)\n",
    "#print(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0a44f",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "110ef1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, load_dataset\n",
    "#training set\n",
    "data=Dataset.from_pandas(prep_train)\n",
    "\n",
    "#test set\n",
    "test_set=Dataset.from_pandas(prep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f4edb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and logger\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "MAX_TOKENS = 10\n",
    "POS_TOKEN = \"<|review_pos|>\"\n",
    "NEG_TOKEN = \"<|review_neg|>\"\n",
    "BOS_TOKENS = [NEG_TOKEN, POS_TOKEN]\n",
    "EOS_TOKEN = \"<|endoftext|>\"\n",
    "PAD_TOKEN = \"<|pad|>\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2',eos_token=EOS_TOKEN,\n",
    "    pad_token=PAD_TOKEN,\n",
    "    max_length=MAX_TOKENS,\n",
    "    is_split_into_words=True)\n",
    "tokenizer.add_tokens(BOS_TOKENS, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "673ca048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 41843\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8892d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f06c3531f3470f8a658f79d77fc352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 70493\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "# texts to numeric vectors of MAX_TOKENS\n",
    "def tokenize_function(examples, tokenizer=tokenizer):\n",
    "    # Add start and end token to each comment\n",
    "    examples = [ex + EOS_TOKEN for ex in examples[\"text\"]]\n",
    "    # tokenizer created input_ids and attention_mask as output\n",
    "    output = tokenizer(\n",
    "        examples,\n",
    "        add_special_tokens=True,  # Only adds pad not eos and bos\n",
    "        max_length=MAX_TOKENS,\n",
    "        truncation=True,\n",
    "        pad_to_max_length=True,\n",
    "    )\n",
    "    # shift labels for next token prediction\n",
    "    # set padding token labels to -100 which is ignored in loss computation\n",
    "    output[\"labels\"] = [x[1:] for x in output[\"input_ids\"]]\n",
    "    output[\"labels\"] = [\n",
    "        [-100 if x == tokenizer.pad_token_id else x for x in y]\n",
    "        for y in output[\"labels\"]\n",
    "    ]\n",
    "    # truncate input ids and attention mask to account for label shift\n",
    "    output[\"input_ids\"] = [x[:-1] for x in output[\"input_ids\"]]\n",
    "    output[\"attention_mask\"] = [x[:-1] for x in output[\"attention_mask\"]]\n",
    "    return output\n",
    "\n",
    "\n",
    "data = data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1adccfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c05afd053f4007be68dc9d5def5c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 41843\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "test_set = test_set.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3e6df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 22:09:44.058966: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-20 22:09:44.059596: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# prepare for use in tensorflow\n",
    "train_tensor_inputs = tf.convert_to_tensor(data[\"input_ids\"])\n",
    "train_tensor_labels = tf.convert_to_tensor(data[\"labels\"])\n",
    "train_tensor_mask = tf.convert_to_tensor(data[\"attention_mask\"])\n",
    "train = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"input_ids\": train_tensor_inputs, \"attention_mask\": train_tensor_mask},\n",
    "        train_tensor_labels,\n",
    "    )\n",
    ")\n",
    "\n",
    "test_tensor_inputs = tf.convert_to_tensor(test_set[\"input_ids\"])\n",
    "test_tensor_inputs = tf.convert_to_tensor(test_set[\"labels\"])\n",
    "test_tensor_inputs = tf.convert_to_tensor(test_set[\"attention_mask\"])\n",
    "test = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"input_ids\": test_tensor_inputs, \"attention_mask\": test_tensor_inputs},\n",
    "        test_tensor_inputs,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0089f7",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6645018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "BATCH_SIZE_PER_REPLICA = 28\n",
    "EPOCHS = numEpochs\n",
    "INITAL_LEARNING_RATE = 0.001\n",
    "try:\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA\n",
    "except NameError as e:\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA\n",
    "BUFFER_SIZE = len(train)\n",
    "\n",
    "# prepare data for consumption\n",
    "train_ds = (train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True))\n",
    "test_ds = test.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b1877d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 124442112 \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124,442,112\n",
      "Trainable params: 124,442,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Drecreasing learning rate scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    INITAL_LEARNING_RATE,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.7,\n",
    "    staircase=True)\n",
    "\n",
    "# initialize model, use_cache=False important! else wrong shape at loss calc\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        use_cache=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdd07db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training when validation acc starts dropping\n",
    "# Save checkpoint of model after each period\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "\n",
    "PATH_BASE = \"/TML_GPT2\"\n",
    "# Create callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", verbose=1, patience=1, restore_best_weights=True\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3945b748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params:\n",
      "batch_size: 28\n",
      "Epochs: 5\n",
      "Step p. Epoch: 2517\n",
      "Initial Learning rate: 0.001\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 22:09:51.522107: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-12-20 22:09:51.527535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872/2517 [=====================>........] - ETA: 2:13 - loss: 2.2575 - accuracy: 0.6387"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3x/q809tzz55jq3vvvgwlrjq6kr0000gn/T/ipykernel_27070/120470194.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34mf\"Initial Learning rate: {INITAL_LEARNING_RATE}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m hist = model.fit(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         ):\n\u001b[1;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2494\u001b[0m       (graph_function,\n\u001b[1;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1861\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "steps_per_epoch = int(BUFFER_SIZE // BATCH_SIZE)\n",
    "print(\n",
    "    f\"Model Params:\\nbatch_size: {BATCH_SIZE}\\nEpochs: {EPOCHS}\\n\"\n",
    "    f\"Step p. Epoch: {steps_per_epoch}\\n\"\n",
    "    f\"Initial Learning rate: {INITAL_LEARNING_RATE}\"\n",
    ")\n",
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11223e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pd.DataFrame(\n",
    "    {\"train loss\": hist.history[\"loss\"], \"test loss\": hist.history[\"val_loss\"]}\n",
    ").melt()\n",
    "loss[\"epoch\"] = loss.groupby(\"variable\").cumcount() + 1\n",
    "sns.lineplot(x=\"epoch\", y=\"value\", hue=\"variable\", data=loss).set(\n",
    "    title=\"Model loss\",\n",
    "    ylabel=\"\",\n",
    "    xticks=range(1, loss[\"epoch\"].max() + 1),\n",
    "    xticklabels=loss[\"epoch\"].unique(),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf72bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.transformer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3383bf6",
   "metadata": {},
   "source": [
    "## Computing the exposure metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_CD=tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fee79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return(np.exp(x)/np.exp(x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test=tokenizer(xt_CD[6][0], return_tensors=\"tf\")\n",
    "print(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(x,model,dictionary,size,index):\n",
    "    #for a in range(0,len(x)): \n",
    "        #This chuck of code will retreive the prediction score in logits from the model\n",
    "    inputs=tokenizer(x[index][0], return_tensors=\"tf\")\n",
    "    outputs = model(inputs)\n",
    "    p0=outputs[0][:, -1, :]\n",
    "       \n",
    "    #now transfrom logits to probabilities using softmax\n",
    "    p0=p0.numpy()  #transform to numpy for easier handling\n",
    "    p0=softmax(p0)\n",
    "        \n",
    "        #join these probabilities to the vocabulary dictionnary\n",
    "    new_dic={}\n",
    "    for (key, value), num in zip(dictionary.items(), p0[0]): #ATTENTION: the index of [0] is to extract from cell, so we musnt change it\n",
    "        new_dic.update({key:num})\n",
    "        \n",
    "        #here we create numericprobs vec which is vector of length 100 which is the numbers that we can have in random secre\n",
    "    numericProbs_gpt = np.zeros((size), dtype = float)\n",
    "\n",
    "        ############################################################################################################\n",
    "        \n",
    "        #append probabilities to vocab dictionnary\n",
    "        #here we will find each number's value between 0 to 99 in the dct and then we will find its prob in p0 to be the next predicted word\n",
    "        # so in numeric probs we have a vector of length 100 and probs for each number to be the next word\n",
    "    for j in range(size):\n",
    "        numericProbs_gpt[j] = new_dic[str(j)]\n",
    "        \n",
    "    return numericProbs_gpt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CALCULATE EXPOSURE ====================================\n",
    "\n",
    "print(\"calculating exposure...\")\n",
    "\n",
    "# 6.1 ENUMERATE OVER EVERY POSSIBLE SECRET -----------------\n",
    "#why start is this?\n",
    "start = len(xt_CD)-secretLength*(numDistinctValues**secretLength)\n",
    "\n",
    "#here we have 100 vectors of len 100\n",
    "p0 = np.ones((numDistinctValues, numDistinctValues), dtype = float)\n",
    "for i in range(start, len(xt_CD), 2 * numDistinctValues):\n",
    "    #print(i)\n",
    "    #this k creates 0 to 99 index\n",
    "    k = int((i-start) / (2 * numDistinctValues))\n",
    "    #print(k)\n",
    "    #here in this for loop we will fill each of 100 vectors by numeric probs function\n",
    "    #what is the values in numeric probs?\n",
    "    #here in p0 we have 100 probs value for 100 possible numbers (from 0 to 99) that can be the next prediction\n",
    "    p0[k] = get_prob(xt_CD, model, dct_CD,numDistinctValues, i)\n",
    "    # this is the prediction for the next index\n",
    "    # we know that a secret devide into two parts for prediction, once the first numeric value then the second\n",
    "    p1 = get_prob(xt_CD, model, dct_CD,numDistinctValues,i + 1)\n",
    "    # then here we have prob for a combination each time, what is the prob of observing 09 18 for example\n",
    "    #the len would be 10000 since the we set numDistinctValues to 100\n",
    "    # we have prob for each combination here to see after the phrase \"my permanent code is\"\n",
    "    p0[k] = p0[k][k] * p1\n",
    "\n",
    "    #then we sort these \n",
    "scoresRaw = np.argsort(p0, None)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb25925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 CALCULATE RANKS OF ALL SECRETS -----------------------\n",
    "d = []\n",
    "for i in range(len(scoresRaw)):\n",
    "    d.append({'rank' : i + 1,\n",
    "              'secret1' : int(scoresRaw[i] / numDistinctValues),\n",
    "              'secret2' : scoresRaw[i] % numDistinctValues,\n",
    "              'secretActual1' : int(insertedSecret.split()[-2]),\n",
    "              'secretActual2' : int(insertedSecret.split()[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d89bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "#6.3 CALCULATE EXPOSURE OF INSERTED SECRET ----------------\n",
    "secretRanks = pd.DataFrame(d)\n",
    "secretMatch1 = secretRanks[secretRanks.secret1 == secretRanks.secretActual1]\n",
    "secretMatch2 = int(secretMatch1[secretMatch1.secret2 == secretMatch1.secretActual2]['rank'])\n",
    "\n",
    "exposure = log(bigR, 2) - log(secretMatch2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642acaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c713c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "secretRanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da7b46",
   "metadata": {},
   "source": [
    "## Computing the baseline for the email dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ddad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need for finetuning with the training data, only need the test set\n",
    "#fetch new model and tokenizer\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "tokenizer_raw = AutoTokenizer.from_pretrained('gpt2')\n",
    "model_raw = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CALCULATE EXPOSURE ====================================\n",
    "\n",
    "print(\"calculating exposure...\")\n",
    "\n",
    "# 6.1 ENUMERATE OVER EVERY POSSIBLE SECRET -----------------\n",
    "#why start is this?\n",
    "start = len(xt_CD)-secretLength*(numDistinctValues**secretLength)\n",
    "\n",
    "#here we have 100 vectors of len 100\n",
    "p0 = np.ones((numDistinctValues, numDistinctValues), dtype = float)\n",
    "for i in range(start, len(xt_CD), 2 * numDistinctValues):\n",
    "    #print(i)\n",
    "    #this k creates 0 to 99 index\n",
    "    k = int((i-start) / (2 * numDistinctValues))\n",
    "    #print(k)\n",
    "    #here in this for loop we will fill each of 100 vectors by numeric probs function\n",
    "    #what is the values in numeric probs?\n",
    "    #here in p0 we have 100 probs value for 100 possible numbers (from 0 to 99) that can be the next prediction\n",
    "    p0[k] = get_prob(xt_CD, model_raw, dct_CD,numDistinctValues, i)\n",
    "    # this is the prediction for the next index\n",
    "    # we know that a secret devide into two parts for prediction, once the first numeric value then the second\n",
    "    p1 = get_prob(xt_CD, model_raw, dct_CD,numDistinctValues,i + 1)\n",
    "    # then here we have prob for a combination each time, what is the prob of observing 09 18 for example\n",
    "    #the len would be 10000 since the we set numDistinctValues to 100\n",
    "    # we have prob for each combination here to see after the phrase \"my permanent code is\"\n",
    "    p0[k] = p0[k][k] * p1\n",
    "\n",
    "    #then we sort these \n",
    "scoresRaw = np.argsort(p0, None)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2085aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 CALCULATE RANKS OF ALL SECRETS -----------------------\n",
    "d = []\n",
    "for i in range(len(scoresRaw)):\n",
    "    d.append({'rank' : i + 1,\n",
    "              'secret1' : int(scoresRaw[i] / numDistinctValues),\n",
    "              'secret2' : scoresRaw[i] % numDistinctValues,\n",
    "              'secretActual1' : int(insertedSecret.split()[-2]),\n",
    "              'secretActual2' : int(insertedSecret.split()[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "#6.3 CALCULATE EXPOSURE OF INSERTED SECRET ----------------\n",
    "secretRanks = pd.DataFrame(d)\n",
    "secretMatch1 = secretRanks[secretRanks.secret1 == secretRanks.secretActual1]\n",
    "secretMatch2 = int(secretMatch1[secretMatch1.secret2 == secretMatch1.secretActual2]['rank'])\n",
    "\n",
    "exposure = log(bigR, 2) - log(secretMatch2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8368b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
